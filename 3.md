# attention-unet
attention unet的实现  

图源：https://speech.ee.ntu.edu.tw/~hylee/ml/2021-spring.html  
特此感谢李宏毅老师！对Attention以及Transformer的讲解清晰易懂

![self-attention实现示意图](/img/3-1.jpg)  
I：Input，形状为N×H×W×C_i  
Wq：Query卷积，采用1×1卷积核  
Wk：Key卷积，采用1×1卷积核  
Wv：Value卷积，采用1×1卷积核  
Q：Query矩阵，经过Query卷积后的结果，形状为N×H×W×C_qk  
K：Key矩阵，经过Key卷积后的结果，形状为N×H×W×C_qk  
V：Value矩阵，经过Value卷积后的结果，形状为N×H×W×C_v  
![QKV相乘顺序](/img/3-2.jpg)  
每个元素展开，相乘过程为：  
![I到O](/img/3-3.jpg)  
可将output视为：  
b1 = q1k1(v1d1) + q1k2(v2d1) + q1(k3v3d1)  
input与output的相同元素为q1  
即  
q为计算相关度中的本格元素，  
k为计算相关度中的其他元素，  
v为计算最终值中的实际有效值元素。 
